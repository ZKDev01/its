{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83e\udde0 Tree of Thoughts con Gemma v\u00eda Ollama\n", "Este notebook permite ejecutar pruebas de razonamiento tipo Tree-of-Thoughts (ToT) con el modelo `gemma3:1b` usando Ollama. Puedes introducir una pregunta o tema, generar ramificaciones, elegir manualmente el mejor camino y guardar el resultado como `.json`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from langchain_ollama import OllamaLLM\n", "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n", "from langchain_core.messages import HumanMessage, AIMessage\n", "import json\n", "from datetime import datetime\n", "\n", "# Clase base para usar gemma con memoria\n", "class Chatbot:\n", "    def __init__(self, base_prompt, model_name=\"gemma3:1b\", temperature=0.8):\n", "        self.base_prompt = base_prompt\n", "        self.chat_prompt = ChatPromptTemplate.from_messages([\n", "            ('system', self.base_prompt),\n", "            MessagesPlaceholder(variable_name='memory'),\n", "            ('human', '{input}')\n", "        ])\n", "        self.llm = OllamaLLM(model=model_name, temperature=temperature)\n", "        self.memory = []\n", "        self.pipeline = self.chat_prompt | self.llm\n", "\n", "    def __call__(self, prompt):\n", "        response = self.pipeline.invoke({\n", "            'input': HumanMessage(prompt),\n", "            'memory': self.memory\n", "        })\n", "        self.memory.append(HumanMessage(content=prompt))\n", "        self.memory.append(AIMessage(content=response))\n", "        return response.content"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Configuraci\u00f3n inicial\n", "tema = \"\u00bfPor qu\u00e9 es importante el an\u00e1lisis matem\u00e1tico en ciencia de datos?\"\n", "ramas_por_paso = 3\n", "num_pasos = 2\n", "\n", "chatbot = Chatbot(\n", "    base_prompt=\"Eres un tutor inteligente que razona paso a paso antes de responder preguntas acad\u00e9micas. Responde en espa\u00f1ol.\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Ejecuci\u00f3n del ciclo ToT\n", "pasos = []\n", "razonamiento_actual = tema\n", "\n", "for paso in range(1, num_pasos+1):\n", "    print(f\"\\n\ud83e\udde0 Paso {paso}: Generando {ramas_por_paso} pensamientos para: {razonamiento_actual}\\n\")\n", "    opciones = []\n", "    for i in range(ramas_por_paso):\n", "        pensamiento = chatbot(f\"Prop\u00f3n una idea para: {razonamiento_actual}\")\n", "        print(f\"{i}. {pensamiento}\\n\")\n", "        opciones.append(pensamiento)\n", "\n", "    seleccion = int(input(f\"Selecciona el \u00edndice del mejor pensamiento para el paso {paso}: \"))\n", "    razonamiento_actual = opciones[seleccion]\n", "    pasos.append({\"paso\": paso, \"opciones\": opciones, \"seleccion\": seleccion})"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Generar respuesta final a partir del razonamiento elegido\n", "respuesta_final = chatbot(f\"Redacta una respuesta completa basada en esta idea: {razonamiento_actual}\")\n", "print(\"\\n\ud83d\udcd8 Respuesta Final:\\n\")\n", "print(respuesta_final)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Guardar resultados\n", "resultado = {\n", "    \"tema\": tema,\n", "    \"pasos\": pasos,\n", "    \"resultado_final\": respuesta_final,\n", "    \"timestamp\": datetime.now().isoformat()\n", "}\n", "\n", "filename = f\"resultados_tot_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n", "with open(filename, \"w\", encoding=\"utf-8\") as f:\n", "    json.dump(resultado, f, indent=2, ensure_ascii=False)\n", "\n", "print(f\"\\n\u2705 Resultado guardado en {filename}\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}