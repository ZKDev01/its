{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83d\udcca An\u00e1lisis Comparativo de Resultados\n", "Este notebook consolida los resultados guardados en archivos `.json` generados por los distintos notebooks del proyecto y produce al menos **dos gr\u00e1ficos por an\u00e1lisis** comparativo.\n", "\n", "Se incluyen an\u00e1lisis como:\n", "- ToT vs RAG (evaluaci\u00f3n)\n", "- Study-Plan Generator vs AI Tutor\n", "- Robustez del generador (fallos por iteraci\u00f3n)\n", "- Iteraciones en MCTS vs Calidad de respuesta\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import json\n", "import os\n", "from pathlib import Path\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "\n", "sns.set(style=\"whitegrid\")\n", "\n", "RESULTS_DIR = Path(\"../test\")\n", "files = list(RESULTS_DIR.glob(\"*.json\"))\n", "print(\"Archivos de resultados detectados:\")\n", "for f in files:\n", "    print(\"-\", f.name)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Cargar todos los archivos de resultados\n", "data = {}\n", "for f in files:\n", "    try:\n", "        with open(f, \"r\", encoding=\"utf-8\") as file:\n", "            contenido = json.load(file)\n", "            data[f.name] = contenido\n", "    except Exception as e:\n", "        print(f\"Error cargando {f.name}: {e}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udcd8 Comparaci\u00f3n ToT vs RAG (evaluaci\u00f3n por JuezIA)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Buscar archivo que contenga comparacion_tot_vs_rag\n", "import pandas as pd\n", "\n", "for nombre, contenido in data.items():\n", "    if \"tot_vs_rag\" in nombre:\n", "        df = pd.DataFrame([\n", "            {\"m\u00e9todo\": \"ToT\", \"evaluacion\": contenido['tot']['evaluacion']},\n", "            {\"m\u00e9todo\": \"RAG\", \"evaluacion\": contenido['rag']['evaluacion']}\n", "        ])\n", "        print(df)\n", "        plt.figure(figsize=(8, 4))\n", "        sns.barplot(x=\"m\u00e9todo\", y=df.index, data=df)\n", "        plt.title(\"Comparaci\u00f3n ToT vs RAG (JuezIA - longitud del texto evaluativo)\")\n", "        plt.ylabel(\"Longitud de evaluaci\u00f3n\")\n", "        plt.xlabel(\"M\u00e9todo\")\n", "        plt.bar(df['m\u00e9todo'], df['evaluacion'].apply(lambda x: len(x)))\n", "        plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udcda Comparaci\u00f3n Study-Plan Generator vs AI Tutor"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Buscar archivo de comparaci\u00f3n de planes de estudio\n", "planes = []\n", "for nombre, contenido in data.items():\n", "    if \"comparacion_planes\" in nombre:\n", "        for item in contenido:\n", "            planes.append({\n", "                \"m\u00e9todo\": item[\"metodo\"],\n", "                \"evaluacion\": item[\"evaluacion\"]\n", "            })\n", "\n", "df_planes = pd.DataFrame(planes)\n", "plt.figure(figsize=(8, 4))\n", "sns.countplot(data=df_planes, x=\"m\u00e9todo\")\n", "plt.title(\"Frecuencia de planes generados por m\u00e9todo\")\n", "plt.show()\n", "\n", "df_planes[\"longitud\"] = df_planes[\"evaluacion\"].apply(lambda x: len(x))\n", "plt.figure(figsize=(8, 4))\n", "sns.boxplot(data=df_planes, x=\"m\u00e9todo\", y=\"longitud\")\n", "plt.title(\"Distribuci\u00f3n de longitud de evaluaci\u00f3n por m\u00e9todo\")\n", "plt.ylabel(\"Longitud de evaluaci\u00f3n\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83e\uddea Robustez del Study-Plan Generator (fallos vs intentos)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Cargar simulaci\u00f3n de fallos\n", "for nombre, contenido in data.items():\n", "    if \"fallos_study_plan\" in nombre:\n", "        intentos = contenido[\"historial_intentos\"]\n", "        plt.figure(figsize=(8, 4))\n", "        sns.histplot(intentos, bins=range(1, max(intentos)+2), kde=False)\n", "        plt.title(\"Histograma de intentos por generaci\u00f3n v\u00e1lida\")\n", "        plt.xlabel(\"Intentos hasta plan v\u00e1lido\")\n", "        plt.ylabel(\"Frecuencia\")\n", "        plt.show()\n", "\n", "        plt.figure(figsize=(6, 4))\n", "        sns.boxplot(x=intentos)\n", "        plt.title(\"Distribuci\u00f3n de intentos\")\n", "        plt.xlabel(\"Intentos\")\n", "        plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83e\udde0 Iteraciones en MCTS vs Evaluaci\u00f3n de Calidad"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualizar calidad en funci\u00f3n de iteraciones\n", "mcts_resultados = []\n", "for nombre, contenido in data.items():\n", "    if \"mcts_iteraciones\" in nombre:\n", "        for item in contenido:\n", "            mcts_resultados.append({\n", "                \"iteraciones\": item[\"iteraciones\"],\n", "                \"evaluacion_len\": len(item[\"evaluacion\"])\n", "            })\n", "\n", "df_mcts = pd.DataFrame(mcts_resultados)\n", "\n", "plt.figure(figsize=(8, 4))\n", "sns.barplot(data=df_mcts, x=\"iteraciones\", y=\"evaluacion_len\")\n", "plt.title(\"Evaluaci\u00f3n (longitud) vs iteraciones MCTS\")\n", "plt.ylabel(\"Longitud de la evaluaci\u00f3n del juez\")\n", "plt.xlabel(\"Iteraciones de MCTS\")\n", "plt.show()\n", "\n", "plt.figure(figsize=(6, 4))\n", "sns.lineplot(data=df_mcts, x=\"iteraciones\", y=\"evaluacion_len\", marker=\"o\")\n", "plt.title(\"Curva de mejora aparente por iteraciones\")\n", "plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}