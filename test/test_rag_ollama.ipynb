{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83e\uddea RAG con LLM ligero usando arquitectura del proyecto\n", "Este notebook usa los m\u00f3dulos definidos en la carpeta `src/` y `create_database.py` desde la ra\u00edz del proyecto.\n", "\n", "Se realiza recuperaci\u00f3n basada en embeddings y generaci\u00f3n con LLM `gemma3:1b` v\u00eda Ollama."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Configuraci\u00f3n de entorno para imports relativos\n", "import sys\n", "from pathlib import Path\n", "\n", "# A\u00f1adir ra\u00edz del proyecto y src al path\n", "BASE_DIR = Path(\"..\").resolve()\n", "SRC_DIR = BASE_DIR / \"src\"\n", "sys.path.append(str(BASE_DIR))\n", "sys.path.append(str(SRC_DIR))\n", "\n", "from data_processing import convert_to_md_using_docling\n", "from graph_db import GraphDB\n", "from chatbot import Chatbot\n", "import create_database  # Este se usar\u00e1 si hay procesamiento de chunks\n", "\n", "from langchain_ollama import OllamaEmbeddings\n", "from langchain.vectorstores import Chroma\n", "from langchain_core.documents import Document\n", "from langchain.text_splitter import RecursiveCharacterTextSplitter\n", "from datetime import datetime\n", "import json"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Par\u00e1metros de prueba\n", "query = \"\u00bfPor qu\u00e9 es importante el an\u00e1lisis matem\u00e1tico en ciencia de datos?\"\n", "n_docs = 3\n", "\n", "# Documentos de prueba localmente embebidos\n", "raw_docs = [\n", "    \"El an\u00e1lisis matem\u00e1tico proporciona herramientas fundamentales para el modelado y la optimizaci\u00f3n de algoritmos en ciencia de datos.\",\n", "    \"Sin c\u00e1lculo diferencial e integral, ser\u00eda imposible entender c\u00f3mo cambian los valores en una red neuronal.\",\n", "    \"Los fundamentos de \u00e1lgebra lineal son esenciales para el manejo de vectores, matrices y transformaciones en espacios de alta dimensi\u00f3n.\",\n", "    \"Estad\u00edstica y probabilidad tambi\u00e9n se consideran parte del an\u00e1lisis matem\u00e1tico aplicado, especialmente en modelos predictivos.\"\n", "]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Indexar documentos usando OllamaEmbeddings y Chroma\n", "embedding = OllamaEmbeddings(model=\"gemma:2b\")\n", "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n", "docs = [Document(page_content=chunk) for doc in raw_docs for chunk in text_splitter.split_text(doc)]\n", "vectorstore = Chroma.from_documents(documents=docs, embedding=embedding, persist_directory=\".chroma_index\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Recuperaci\u00f3n de documentos relevantes\n", "retrieved_docs = vectorstore.similarity_search(query, k=n_docs)\n", "contexto = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n", "print(\"\ud83d\udcc4 Documentos recuperados:\\n\")\n", "for i, doc in enumerate(retrieved_docs):\n", "    print(f\"[{i}] {doc.page_content}\\n\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Generar respuesta usando Chatbot definido en src/chatbot.py\n", "chatbot = Chatbot(\n", "    base_prompt=\"Eres un tutor acad\u00e9mico que responde en espa\u00f1ol bas\u00e1ndote en contexto recuperado.\"\n", ")\n", "\n", "prompt = f\"\"\"\n", "Contexto:\n", "{contexto}\n", "\n", "Pregunta:\n", "{query}\n", "\"\"\"\n", "\n", "respuesta = chatbot(prompt)\n", "print(\"\\n\ud83d\udcd8 Respuesta generada:\\n\")\n", "print(respuesta)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Guardar resultado\n", "resultado = {\n", "    \"query\": query,\n", "    \"contexto\": contexto,\n", "    \"respuesta\": respuesta,\n", "    \"timestamp\": datetime.now().isoformat()\n", "}\n", "\n", "filename = f\"resultados_rag_integration_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n", "with open(filename, \"w\", encoding=\"utf-8\") as f:\n", "    json.dump(resultado, f, indent=2, ensure_ascii=False)\n", "\n", "print(f\"\u2705 Resultado guardado en {filename}\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}