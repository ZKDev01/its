{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ RAG con LLM Ligero usando arquitectura del proyecto\n",
    "Este notebook usa los m√≥dulos definidos en la carpeta `src/` y `create_database.py` desde la ra√≠z del proyecto.\n",
    "\n",
    "Se realiza recuperaci√≥n basada en embeddings y generaci√≥n con LLM `gemma3:1b` v√≠a Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n de entorno para imports relativos\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# A√±adir ra√≠z del proyecto y src al path\n",
    "BASE_DIR = Path(\"..\").resolve()\n",
    "SRC_DIR = BASE_DIR / \"src\"\n",
    "sys.path.append(str(BASE_DIR))\n",
    "sys.path.append(str(SRC_DIR))\n",
    "\n",
    "from data_processing import convert_to_md_using_docling\n",
    "from graph_db import GraphDB\n",
    "from chatbot import Chatbot\n",
    "import create_database  # Este se usar√° si hay procesamiento de chunks\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Par√°metros de prueba\n",
    "query = \"¬øPor qu√© es importante el an√°lisis matem√°tico en ciencia de datos?\"\n",
    "n_docs = 3\n",
    "\n",
    "# Documentos de prueba localmente embebidos\n",
    "raw_docs = [\n",
    "    \"El an√°lisis matem√°tico proporciona herramientas fundamentales para el modelado y la optimizaci√≥n de algoritmos en ciencia de datos.\",\n",
    "    \"Sin c√°lculo diferencial e integral, ser√≠a imposible entender c√≥mo cambian los valores en una red neuronal.\",\n",
    "    \"Los fundamentos de √°lgebra lineal son esenciales para el manejo de vectores, matrices y transformaciones en espacios de alta dimensi√≥n.\",\n",
    "    \"Estad√≠stica y probabilidad tambi√©n se consideran parte del an√°lisis matem√°tico aplicado, especialmente en modelos predictivos.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexar documentos usando OllamaEmbeddings y Chroma\n",
    "embedding = OllamaEmbeddings(model=\"gemma:2b\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "docs = [Document(page_content=chunk) for doc in raw_docs for chunk in text_splitter.split_text(doc)]\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=embedding, persist_directory=\".chroma_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperaci√≥n de documentos relevantes\n",
    "retrieved_docs = vectorstore.similarity_search(query, k=n_docs)\n",
    "contexto = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "print(\"üìÑ Documentos recuperados:\\n\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"[{i}] {doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar respuesta usando Chatbot definido en src/chatbot.py\n",
    "chatbot = Chatbot(\n",
    "    base_prompt=\"Eres un tutor acad√©mico que responde en espa√±ol bas√°ndote en contexto recuperado.\"\n",
    ")\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Contexto:\n",
    "{contexto}\n",
    "\n",
    "Pregunta:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "respuesta = chatbot(prompt)\n",
    "print(\"\\nüìò Respuesta generada:\\n\")\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultado\n",
    "resultado = {\n",
    "    \"query\": query,\n",
    "    \"contexto\": contexto,\n",
    "    \"respuesta\": respuesta,\n",
    "    \"timestamp\": datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "filename = f\"resultados_rag_integration_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(resultado, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ Resultado guardado en {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
