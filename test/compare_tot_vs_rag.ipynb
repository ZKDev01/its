{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83e\udd16 Comparaci\u00f3n ToT vs RAG con JuezIA\n", "Este notebook permite comparar los m\u00e9todos de generaci\u00f3n **Tree-of-Thoughts (ToT)** y **RAG (Retrieval-Augmented Generation)** usando `gemma3:1b` v\u00eda Ollama, con evaluaci\u00f3n autom\u00e1tica por un modelo juez (`JuezIA`)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Configuraci\u00f3n de entorno para imports relativos\n", "import sys\n", "from pathlib import Path\n", "BASE_DIR = Path(\"..\").resolve()\n", "SRC_DIR = BASE_DIR / \"src\"\n", "sys.path.append(str(BASE_DIR))\n", "sys.path.append(str(SRC_DIR))\n", "\n", "from chatbot import Chatbot\n", "from langchain_ollama import OllamaEmbeddings\n", "from langchain.vectorstores import Chroma\n", "from langchain_core.documents import Document\n", "from langchain.text_splitter import RecursiveCharacterTextSplitter\n", "from datetime import datetime\n", "import json"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Inicializar chatbot y juez (gemma3 como tutor y juez)\n", "tutor = Chatbot(\"Eres un tutor inteligente que razona paso a paso en espa\u00f1ol.\")\n", "juez = Chatbot(\"Eres un cr\u00edtico educativo. Eval\u00faa si la respuesta del tutor es clara, precisa y \u00fatil.\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Tema a comparar\n", "tema = \"\u00bfPor qu\u00e9 es importante el an\u00e1lisis matem\u00e1tico en ciencia de datos?\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Simulaci\u00f3n de ToT cl\u00e1sico (simplificado para demostraci\u00f3n)\n", "pensamientos = [\n", "    tutor(f\"Prop\u00f3n una idea sobre: {tema}\"),\n", "    tutor(f\"Otra idea sobre: {tema}\"),\n", "    tutor(f\"Una \u00faltima idea sobre: {tema}\")\n", "]\n", "print(\"Pensamientos generados (ToT):\\n\")\n", "for i, p in enumerate(pensamientos):\n", "    print(f\"{i}. {p}\\n\")\n", "\n", "seleccion = int(input(\"Selecciona el mejor pensamiento (\u00edndice): \"))\n", "mejor_pensamiento = pensamientos[seleccion]\n", "respuesta_tot = tutor(f\"Redacta una respuesta basada en esta idea: {mejor_pensamiento}\")\n", "print(\"\\n\ud83d\udcd8 Respuesta ToT:\\n\", respuesta_tot)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Simulaci\u00f3n de RAG con base embebida\n", "raw_docs = [\n", "    \"El an\u00e1lisis matem\u00e1tico proporciona herramientas fundamentales para el modelado y la optimizaci\u00f3n de algoritmos en ciencia de datos.\",\n", "    \"Sin c\u00e1lculo diferencial e integral, ser\u00eda imposible entender c\u00f3mo cambian los valores en una red neuronal.\",\n", "    \"Los fundamentos de \u00e1lgebra lineal son esenciales para el manejo de vectores, matrices y transformaciones en espacios de alta dimensi\u00f3n.\",\n", "    \"Estad\u00edstica y probabilidad tambi\u00e9n se consideran parte del an\u00e1lisis matem\u00e1tico aplicado, especialmente en modelos predictivos.\"\n", "]\n", "\n", "embedding = OllamaEmbeddings(model=\"gemma:2b\")\n", "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n", "docs = [Document(page_content=chunk) for doc in raw_docs for chunk in text_splitter.split_text(doc)]\n", "vectorstore = Chroma.from_documents(documents=docs, embedding=embedding, persist_directory=\".rag_comparacion_index\")\n", "\n", "docs_recuperados = vectorstore.similarity_search(tema, k=3)\n", "contexto_rag = \"\\n\\n\".join([doc.page_content for doc in docs_recuperados])\n", "prompt_rag = f\"Contexto:\\n{contexto_rag}\\n\\nPregunta:\\n{tema}\"\n", "respuesta_rag = tutor(prompt_rag)\n", "print(\"\\n\ud83d\udcd8 Respuesta RAG:\\n\", respuesta_rag)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Evaluaci\u00f3n autom\u00e1tica con JuezIA\n", "evaluacion_tot = juez(f\"Pregunta: {tema}\\nRespuesta del tutor: {respuesta_tot}\\n\u00bfLa explicaci\u00f3n es clara y \u00fatil? Justifica.\")\n", "evaluacion_rag = juez(f\"Pregunta: {tema}\\nRespuesta del tutor: {respuesta_rag}\\n\u00bfLa explicaci\u00f3n es clara y \u00fatil? Justifica.\")\n", "\n", "print(\"\\n\u2696\ufe0f Evaluaci\u00f3n ToT:\\n\", evaluacion_tot)\n", "print(\"\\n\u2696\ufe0f Evaluaci\u00f3n RAG:\\n\", evaluacion_rag)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Guardar comparaci\u00f3n\n", "resultado = {\n", "    \"tema\": tema,\n", "    \"tot\": {\n", "        \"respuesta\": respuesta_tot,\n", "        \"evaluacion\": evaluacion_tot\n", "    },\n", "    \"rag\": {\n", "        \"respuesta\": respuesta_rag,\n", "        \"evaluacion\": evaluacion_rag\n", "    },\n", "    \"timestamp\": datetime.now().isoformat()\n", "}\n", "\n", "filename = f\"comparacion_tot_vs_rag_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n", "with open(filename, \"w\", encoding=\"utf-8\") as f:\n", "    json.dump(resultado, f, indent=2, ensure_ascii=False)\n", "\n", "print(f\"\\n\u2705 Resultado guardado en {filename}\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}