{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d5268ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\" \n",
    "Eres un asistente especializado capaz de responder preguntas sencillas\n",
    "\"\"\"\n",
    "\n",
    "DECOMPOSITION_SYSTEM_PROMPT = \"\"\" \n",
    "Eres un experto en descomposición de problemas. Tu tarea es tomar un problema complejo y dividirlo en subproblemas más pequeños y manejables.\n",
    "\n",
    "Tener en cuenta:\n",
    "- Cada subproblema debe ser específico y manejable\n",
    "- Los subproblemas deben seguir un orden lógico\n",
    "- Juntos, los subproblemas deben cubrir completamente el problema original\n",
    "\n",
    "Formato de respuesta:\n",
    "- Responde ÚNICAMENTE con una lista de subproblemas\n",
    "- Un subproblema por línea\n",
    "- Usa viñetas (`-`)\n",
    "\n",
    "Ejemplo de salida:\n",
    "- subproblema 1\n",
    "- subproblema 2\n",
    "- subproblema 3\n",
    "\"\"\"\n",
    "\n",
    "SELECT_PROMPT = \"\"\" \n",
    "Eres un experto en clasificar si una entrada de usuario es un subproblema de un problema mucho mayor. \n",
    "\n",
    "Formato de respuesta: \n",
    "- Responde ÚNICAMENTE con TRUE o FALSE\n",
    "\"\"\"\n",
    "\n",
    "CONSTRUCT_PROMPT = \"\"\" \n",
    "Eres un experto en crear preguntas. Tu tarea es tomar una entrada del usuario y generar a partir de esta una pregunta o problema.\n",
    "\n",
    "Instrucciones:\n",
    "- Generar la pregunta sencilla, corta y clara.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96e5dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import sys \n",
    "import json\n",
    "from io import StringIO\n",
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from langchain_ollama import (\n",
    "  ChatOllama,\n",
    "  OllamaLLM,\n",
    "  OllamaEmbeddings\n",
    ")\n",
    "from langchain_core.messages import (\n",
    "  BaseMessage,\n",
    "  AIMessage,\n",
    "  HumanMessage  \n",
    ")\n",
    "from langchain_core.prompts import (\n",
    "  ChatPromptTemplate,\n",
    "  MessagesPlaceholder  \n",
    ")\n",
    "\n",
    "GEMMA = \"gemma3:1b\"             # ollama gemma3:1b\n",
    "DEEPSEEK = \"deepseek-r1:1.5b\"   # ollama deepseek-r1:1.5b\n",
    "\n",
    "class LLM(ABC):\n",
    "  def __init__(self, model:str=\"\", api_key:str=\"\", temperature:float=0.7):\n",
    "    self.model = model\n",
    "    self.api_key = api_key \n",
    "    self.temperature = temperature\n",
    "  \n",
    "  @abstractmethod\n",
    "  def __call__(self, query:str) -> str:\n",
    "    pass \n",
    "\n",
    "class Ollama(LLM):\n",
    "  def __init__(self, model:str=\"\", api_key:str = \"\", temperature:float = 0.7, system_prompt:str = \"\"):\n",
    "    super().__init__(model, api_key, temperature)\n",
    "    \n",
    "    try:\n",
    "      self.llm = ChatOllama(self.model, self.temperature)\n",
    "    except Exception:\n",
    "      self.llm = ChatOllama(model=GEMMA, temperature=0.7)\n",
    "    \n",
    "    self.system_prompt:str = system_prompt if len(system_prompt) > 0 else DEFAULT_SYSTEM_PROMPT\n",
    "    self.memory:List = []\n",
    "    \n",
    "    self.chat_prompt = ChatPromptTemplate.from_messages(\n",
    "      [\n",
    "        ('system', f'{self.system_prompt}'),\n",
    "        MessagesPlaceholder(variable_name='memory'),\n",
    "        ('human', '{query}')\n",
    "      ]\n",
    "    )\n",
    "    self.chain = self.chat_prompt | self.llm  \n",
    "  \n",
    "  def __call__(self, query:str) -> str:\n",
    "    response = self.chain.invoke(\n",
    "      {\n",
    "        \"query\": HumanMessage(query),\n",
    "        \"memory\": self.memory\n",
    "      }\n",
    "    )\n",
    "    self.memory.append(HumanMessage(content=query))\n",
    "    self.memory.append(AIMessage(content=response))\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66056b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_problem(problem:str, verbose:bool=False) -> List[str]:\n",
    "  \"\"\"Descompone un problema complejo en una lista de subproblemas más manejables.\n",
    "\n",
    "  Args:\n",
    "    problem (str): el problema principal a descomponer\n",
    "\n",
    "  Returns:\n",
    "    List[str]: lista de subproblemas que juntos resuelven el problema original\n",
    "  \"\"\"\n",
    "  # crear una instancia temporal para descomposición\n",
    "  decomposer = ChatOllama(model=GEMMA, temperature=0.3) # temperatura más baja para mayor consistencia\n",
    "  decomposition_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', DECOMPOSITION_SYSTEM_PROMPT),\n",
    "    ('human', f'Descompone el siguiente problema en subproblemas:\\n\\n{problem}')\n",
    "  ])\n",
    "  chain = decomposition_prompt | decomposer\n",
    "  response = chain.invoke({\"problem\":problem}).content\n",
    "  \n",
    "  if verbose: print(response)\n",
    "  \n",
    "  subproblems = []\n",
    "  lines = response.strip().split('\\n')\n",
    "  for line in lines:\n",
    "    line = line.strip()\n",
    "    \n",
    "    # limpiar viñetas y numeración\n",
    "    if line.startswith('- '):\n",
    "      subproblem = line[2:].strip()\n",
    "    elif line.startswith('• '):\n",
    "      subproblem = line[2:].strip()\n",
    "    elif re.match(r'^\\d+\\.\\s*', line):\n",
    "      subproblem = re.sub(r'^\\d+\\.\\s*', '', line).strip()\n",
    "    elif line and not line.startswith('#'):\n",
    "      subproblem = line.strip()\n",
    "    else:\n",
    "      continue\n",
    "    \n",
    "    if subproblem and len(subproblem) > 5:   # filtrar lineas muy cortas\n",
    "      subproblems.append(subproblem)\n",
    "    \n",
    "  if not subproblems:\n",
    "    raise Exception(\"ERROR: no se pudo dividir el problema\")\n",
    "    \n",
    "  return subproblems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c00600",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = \"\"\"\n",
    "Descompone el problema siguiente en subproblemas:\n",
    "\n",
    "Haz un analisis completo de cómo funciona la Inteligencia Artificial\n",
    "1. Definición de qué es la inteligencia artificial\n",
    "2. Aplicación\n",
    "3. Ejemplos \n",
    "4. Genera un código sencillo en Python de cómo trabaja la Inteligencia Artificial\n",
    "\"\"\"\n",
    "\n",
    "subproblems = decompose_problem(problem, verbose=False)\n",
    "for i, subproblem in enumerate(subproblems, 1):\n",
    "  print(f\"{i}. {subproblem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d99b23ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_problem(problem:str, subproblem:str, verbose:bool=False) -> bool:\n",
    "  selector = ChatOllama(model=GEMMA, temperature=0.5) # temperatura más baja para mayor consistencia\n",
    "  selection_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', SELECT_PROMPT),\n",
    "    ('human', '{problem}')\n",
    "  ])\n",
    "  chain = selection_prompt | selector\n",
    "  response = chain.invoke({\"problem\":f\"Problema original:\\n\\n{problem}\\n\\nEntrada de usuario:\\n\\n{subproblem}\"}).content\n",
    "  \n",
    "  if verbose: print(response)\n",
    "  \n",
    "  lines = response.strip().split('\\n')\n",
    "  if len(lines) == 1: \n",
    "    try: \n",
    "      output = True if lines[0] == \"TRUE\" else False if lines[0] == \"FALSE\" else None\n",
    "      if output == None:\n",
    "        return False\n",
    "        raise Exception(f\"ERROR al convertir a bool: {response}\")\n",
    "      return output\n",
    "    except:\n",
    "      return False\n",
    "      raise Exception(f\"ERROR al convertir a bool: {response}\")\n",
    "  return False\n",
    "  raise Exception(f\"ERROR al responder adecuadamente: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8fe925",
   "metadata": {},
   "outputs": [],
   "source": [
    "selections = []\n",
    "for subproblem in subproblems:\n",
    "  if select_problem(problem, subproblem, verbose=False):\n",
    "    selections.append(subproblem)\n",
    "\n",
    "[print(f\"{i}. {select}\") for i, select in enumerate(selections, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ef62fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_task(subproblem:str, verbose:bool=False) -> str:\n",
    "  selector = ChatOllama(model=GEMMA, temperature=0.7) # temperatura más baja para mayor consistencia\n",
    "  selection_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', CONSTRUCT_PROMPT),\n",
    "    ('human', 'Entrada del usuario: {problem}')\n",
    "  ])\n",
    "  chain = selection_prompt | selector\n",
    "  response = chain.invoke({\"problem\":subproblem}).content\n",
    "  \n",
    "  if verbose: print(response)\n",
    "  \n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba73668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = []\n",
    "for select in selections:\n",
    "  tasks.append( construct_task(select) ) \n",
    "\n",
    "[print(f\"{i}. {task}\") for i, task in enumerate(tasks, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91eb9225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de subproblemas: 49\n",
      "Cantidad de subproblemas seleccionados: 1\n",
      "1. Okay, here are a few options for responding to the user's input \"Predicción: Realizamos la predicción de los datos con el modelo entrenado,\" ranging from straightforward to slightly more engaging:\n",
      "\n",
      "**Option 1 (Straightforward):**\n",
      "\n",
      "\"That's right!  We're using our model to make predictions.\"\n",
      "\n",
      "**Option 2 (Slightly more informative):**\n",
      "\n",
      "\"Exactly! Our model is currently generating predictions based on the data you provided.\"\n",
      "\n",
      "**Option 3 (A bit more engaging):**\n",
      "\n",
      "\"Yep!  We're leveraging our model to offer insights and predictions.\"\n",
      "\n",
      "**Option 4 (If you want to offer a little more context):**\n",
      "\n",
      "\"Correct!  We're using our model to analyze the data and generate predictions for [briefly mention the data, e.g., sales trends, customer behavior].\"\n",
      "\n",
      "\n",
      "**To help me refine the response even further, could you tell me:**\n",
      "\n",
      "*   **What kind of model is it?** (e.g., machine learning, statistical analysis, etc.)\n",
      "*   **What kind of predictions is it making?** (e.g., sales, marketing, risk assessment?)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem = \"\"\"\n",
    "Haz un analisis completo de cómo funciona la Inteligencia Artificial\n",
    "1. Definición de qué es la inteligencia artificial\n",
    "2. Aplicación\n",
    "3. Ejemplos \n",
    "4. Genera un código sencillo en Python de cómo trabaja la Inteligencia Artificial\n",
    "\"\"\"\n",
    "\n",
    "subproblems = decompose_problem(problem, verbose=False)\n",
    "selections = [ s for s in subproblems if select_problem(problem=problem, subproblem=s) ]\n",
    "tasks = [construct_task(select) for select in selections]\n",
    "\n",
    "print(f\"Cantidad de subproblemas: {len(subproblems)}\")\n",
    "print(f\"Cantidad de subproblemas seleccionados: {len(selections)}\")\n",
    "\n",
    "[print(f\"{i}. {task}\") for i, task in enumerate(tasks, 1)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
