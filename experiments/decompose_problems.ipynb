{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5268ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\" \n",
    "Eres un asistente especializado capaz de responder preguntas sencillas\n",
    "\"\"\"\n",
    "\n",
    "DECOMPOSITION_SYSTEM_PROMPT = \"\"\" \n",
    "Eres un experto en descomposición de problemas. Tu tarea es tomar un problema complejo y dividirlo en subproblemas más pequeños y manejables.\n",
    "\n",
    "Tener en cuenta:\n",
    "- Cada subproblema debe ser específico y manejable\n",
    "- Los subproblemas deben seguir un orden lógico\n",
    "- Juntos, los subproblemas deben cubrir completamente el problema original\n",
    "\n",
    "Formato de respuesta:\n",
    "- Responde ÚNICAMENTE con una lista de subproblemas\n",
    "- Un subproblema por línea\n",
    "- Usa viñetas (`-`)\n",
    "\n",
    "Ejemplo de salida:\n",
    "- subproblema 1\n",
    "- subproblema 2\n",
    "- subproblema 3\n",
    "\"\"\"\n",
    "\n",
    "SELECT_PROMPT = \"\"\" \n",
    "Eres un experto en clasificar si una entrada de usuario es un subproblema de un problema mucho mayor. \n",
    "\n",
    "Formato de respuesta: \n",
    "- Responde ÚNICAMENTE con TRUE o FALSE\n",
    "\"\"\"\n",
    "\n",
    "CONSTRUCT_PROMPT = \"\"\" \n",
    "Eres un experto en crear preguntas. Tu tarea es tomar una entrada del usuario y generar a partir de esta una pregunta o problema.\n",
    "\n",
    "Instrucciones:\n",
    "- Generar la pregunta sencilla, corta y clara.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e5dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import sys \n",
    "import json\n",
    "from io import StringIO\n",
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from langchain_ollama import (\n",
    "  ChatOllama,\n",
    "  OllamaLLM,\n",
    "  OllamaEmbeddings\n",
    ")\n",
    "from langchain_core.messages import (\n",
    "  BaseMessage,\n",
    "  AIMessage,\n",
    "  HumanMessage  \n",
    ")\n",
    "from langchain_core.prompts import (\n",
    "  ChatPromptTemplate,\n",
    "  MessagesPlaceholder  \n",
    ")\n",
    "\n",
    "GEMMA = \"gemma3:1b\"             # ollama gemma3:1b\n",
    "DEEPSEEK = \"deepseek-r1:1.5b\"   # ollama deepseek-r1:1.5b\n",
    "\n",
    "class LLM(ABC):\n",
    "  def __init__(self, model:str=\"\", api_key:str=\"\", temperature:float=0.7):\n",
    "    self.model = model\n",
    "    self.api_key = api_key \n",
    "    self.temperature = temperature\n",
    "  \n",
    "  @abstractmethod\n",
    "  def __call__(self, query:str) -> str:\n",
    "    pass \n",
    "\n",
    "class Ollama(LLM):\n",
    "  def __init__(self, model:str=\"\", api_key:str = \"\", temperature:float = 0.7, system_prompt:str = \"\"):\n",
    "    super().__init__(model, api_key, temperature)\n",
    "    \n",
    "    try:\n",
    "      self.llm = ChatOllama(self.model, self.temperature)\n",
    "    except Exception:\n",
    "      self.llm = ChatOllama(model=GEMMA, temperature=0.7)\n",
    "    \n",
    "    self.system_prompt:str = system_prompt if len(system_prompt) > 0 else DEFAULT_SYSTEM_PROMPT\n",
    "    self.memory:List = []\n",
    "    \n",
    "    self.chat_prompt = ChatPromptTemplate.from_messages(\n",
    "      [\n",
    "        ('system', f'{self.system_prompt}'),\n",
    "        MessagesPlaceholder(variable_name='memory'),\n",
    "        ('human', '{query}')\n",
    "      ]\n",
    "    )\n",
    "    self.chain = self.chat_prompt | self.llm  \n",
    "  \n",
    "  def __call__(self, query:str) -> str:\n",
    "    response = self.chain.invoke(\n",
    "      {\n",
    "        \"query\": HumanMessage(query),\n",
    "        \"memory\": self.memory\n",
    "      }\n",
    "    )\n",
    "    self.memory.append(HumanMessage(content=query))\n",
    "    self.memory.append(AIMessage(content=response))\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66056b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_problem(problem:str, verbose:bool=False) -> List[str]:\n",
    "  \"\"\"Descompone un problema complejo en una lista de subproblemas más manejables.\n",
    "\n",
    "  Args:\n",
    "    problem (str): el problema principal a descomponer\n",
    "\n",
    "  Returns:\n",
    "    List[str]: lista de subproblemas que juntos resuelven el problema original\n",
    "  \"\"\"\n",
    "  # crear una instancia temporal para descomposición\n",
    "  decomposer = ChatOllama(model=GEMMA, temperature=0.2) # temperatura más baja para mayor consistencia\n",
    "  decomposition_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', DECOMPOSITION_SYSTEM_PROMPT),\n",
    "    ('human', f'Descompone el siguiente problema en subproblemas:\\n\\n{problem}')\n",
    "  ])\n",
    "  chain = decomposition_prompt | decomposer\n",
    "  response = chain.invoke({\"problem\":problem}).content\n",
    "  \n",
    "  if verbose: print(response)\n",
    "  \n",
    "  subproblems = []\n",
    "  lines = response.strip().split('\\n')\n",
    "  for line in lines:\n",
    "    line = line.strip()\n",
    "    \n",
    "    # limpiar viñetas y numeración\n",
    "    if line.startswith('- '):\n",
    "      subproblem = line[2:].strip()\n",
    "    elif line.startswith('• '):\n",
    "      subproblem = line[2:].strip()\n",
    "    elif re.match(r'^\\d+\\.\\s*', line):\n",
    "      subproblem = re.sub(r'^\\d+\\.\\s*', '', line).strip()\n",
    "    elif line and not line.startswith('#'):\n",
    "      subproblem = line.strip()\n",
    "    else:\n",
    "      continue\n",
    "    \n",
    "    if subproblem and len(subproblem) > 5:   # filtrar lineas muy cortas\n",
    "      subproblems.append(subproblem)\n",
    "    \n",
    "  if not subproblems:\n",
    "    raise Exception(\"ERROR: no se pudo dividir el problema\")\n",
    "    \n",
    "  return subproblems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c00600",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = \"\"\"\n",
    "Descompone el problema siguiente en subproblemas:\n",
    "\n",
    "Haz un analisis completo de cómo funciona la Inteligencia Artificial\n",
    "1. Definición de qué es la inteligencia artificial\n",
    "2. Aplicación\n",
    "3. Ejemplos \n",
    "4. Genera un código sencillo en Python de cómo trabaja la Inteligencia Artificial\n",
    "\"\"\"\n",
    "\n",
    "subproblems = decompose_problem(problem, verbose=False)\n",
    "for i, subproblem in enumerate(subproblems, 1):\n",
    "  print(f\"{i}. {subproblem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99b23ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_problem(problem:str, subproblem:str, verbose:bool=False) -> bool:\n",
    "  selector = ChatOllama(model=GEMMA, temperature=0.2) # temperatura más baja para mayor consistencia\n",
    "  selection_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', SELECT_PROMPT),\n",
    "    ('human', '{problem}')\n",
    "  ])\n",
    "  chain = selection_prompt | selector\n",
    "  response = chain.invoke({\"problem\":f\"Problema original:\\n\\n{problem}\\n\\nEntrada de usuario:\\n\\n{subproblem}\"}).content\n",
    "  \n",
    "  if verbose: print(response)\n",
    "  \n",
    "  lines = response.strip().split('\\n')\n",
    "  if len(lines) == 1: \n",
    "    try: \n",
    "      output = True if lines[0] == \"TRUE\" else False if lines[0] == \"FALSE\" else None\n",
    "      if output == None:\n",
    "        raise Exception(f\"ERROR al convertir a bool: {response}\")\n",
    "      return output\n",
    "    except:\n",
    "      raise Exception(f\"ERROR al convertir a bool: {response}\")\n",
    "  raise Exception(f\"ERROR al responder adecuadamente: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8fe925",
   "metadata": {},
   "outputs": [],
   "source": [
    "selections = []\n",
    "for subproblem in subproblems:\n",
    "  if select_problem(problem, subproblem, verbose=False):\n",
    "    selections.append(subproblem)\n",
    "\n",
    "[print(f\"{i}. {select}\") for i, select in enumerate(selections, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef62fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_task(subproblem:str, verbose:bool=False) -> str:\n",
    "  selector = ChatOllama(model=GEMMA, temperature=0.5) # temperatura más baja para mayor consistencia\n",
    "  selection_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', CONSTRUCT_PROMPT),\n",
    "    ('human', 'Entrada del usuario: {problem}')\n",
    "  ])\n",
    "  chain = selection_prompt | selector\n",
    "  response = chain.invoke({\"problem\":subproblem}).content\n",
    "  \n",
    "  if verbose: print(response)\n",
    "  \n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba73668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = []\n",
    "for select in selections:\n",
    "  tasks.append( construct_task(select) ) \n",
    "\n",
    "[print(f\"{i}. {task}\") for i, task in enumerate(tasks, 1)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
