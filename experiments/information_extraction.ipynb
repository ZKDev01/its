{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42eec01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import json \n",
    "from typing import List,Tuple,Dict\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(model=\"gemma3:1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9cd8ba",
   "metadata": {},
   "source": [
    "![information_extraction](../resources/information_extraction.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f32579d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text:str, chunk_size:int = 500) -> List[str]:\n",
    "  \"\"\"Chunking: Divide el texto en fragmentos manejables\n",
    "\n",
    "  Args:\n",
    "      text (str): texto para fragmentar\n",
    "      chunk_size (int, optional): tama침o de fragmentos resultantes. Defaults to 500.\n",
    "\n",
    "  Returns:\n",
    "      List[str]: resultado de la operaci칩n\n",
    "  \"\"\"\n",
    "  words = text.split()\n",
    "  chunks = []\n",
    "  current_chunk = []\n",
    "  current_length = 0\n",
    "  \n",
    "  for word in words:\n",
    "    current_chunk.append(word)\n",
    "    current_length += len(word) + 1\n",
    "    \n",
    "    if current_length >= chunk_size:\n",
    "      chunks.append(' '.join(current_chunk))\n",
    "      current_chunk = []\n",
    "      current_length = 0\n",
    "  \n",
    "  if current_chunk:\n",
    "    chunks.append(' '.join(current_chunk))\n",
    "  \n",
    "  return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95994c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coreference_resolution(text:str) -> str:\n",
    "  \"Resuelve pronombres y referencias en el texto\"\n",
    "  prompt = f\"\"\" \n",
    "  Resolve all pronouns and coreferences in the following text. \n",
    "  Replace pronouns (he, she, it, they, etc.) with the actual entities they refer to.\n",
    "  Make the text clear and unambiguous.\n",
    "  \n",
    "  Text: {text}\n",
    "  \n",
    "  Resolved text:\n",
    "  \"\"\"\n",
    "  resolved = llm.invoke(prompt)\n",
    "  return resolved.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67c7c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def named_entity_recognition(text:str) -> List[Dict]:\n",
    "  \"Extrae entidades nombradas del texto\"\n",
    "  prompt = f\"\"\"\n",
    "  Extract all named entities from the following text.\n",
    "  Identify PERSON, ORGANIZATION, LOCATION, EVENT, CONCEPT, and other important entities.\n",
    "  \n",
    "  Format your response as a JSON list of objects with 'entity' and 'type' fields.\n",
    "  \n",
    "  Example format:\n",
    "  [\n",
    "    {{\"entity\": \"John Smith\", \"type\": \"PERSON\"}},\n",
    "    {{\"entity\": \"Microsoft\", \"type\": \"ORGANIZATION\"}},\n",
    "    {{\"entity\": \"New York\", \"type\": \"LOCATION\"}}\n",
    "  ]\n",
    "  \n",
    "  Text: {text}\n",
    "  \n",
    "  Entities (JSON format only):\n",
    "  \"\"\"\n",
    "  response = llm.invoke(prompt)\n",
    "  try:\n",
    "    json_match = re.search(r'\\[.*\\]', response, re.DOTALL)\n",
    "    if json_match:\n",
    "      entities = json.loads(json_match.group())\n",
    "      return entities\n",
    "  except:\n",
    "    pass \n",
    "  \n",
    "  # an치lisis alternativo si JSON falla\n",
    "  entities = []\n",
    "  lines = response.split('\\n')\n",
    "  for line in lines:\n",
    "    if '\"entity\"' in line and '\"type\"' in line:\n",
    "      try:\n",
    "        entity_match = re.search(r'\"entity\":\\s*\"([^\"]+)\"', line)\n",
    "        type_match = re.search(r'\"type\":\\s*\"([^\"]+)\"', line)\n",
    "        if entity_match and type_match:\n",
    "          entities.append({\n",
    "            \"entity\": entity_match.group(1),\n",
    "            \"type\": type_match.group(1)\n",
    "          })\n",
    "      except:\n",
    "        continue\n",
    "  return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452eecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relationship_extraction(text:str, entities:List[Dict]) -> List[Tuple[str,str,str]]:\n",
    "  \"Extrae relaciones entre entidades\"\n",
    "  entity_list = [e['entity'] for e in entities]\n",
    "  entity_str = ', '.join(entity_list)\n",
    "  \n",
    "  prompt = f\"\"\" \n",
    "  Given the following text and list of entities, extract relationships between entities.\n",
    "  \n",
    "  Text: {text}\n",
    "  \n",
    "  Entities: {entity_str}\n",
    "  \n",
    "  Extract relationships in the format: (Entity1, Relationship, Entity2)\n",
    "  \n",
    "  Examples:\n",
    "  - (John Smith, works_for, Microsoft)\n",
    "  - (Microsoft, located_in, Seattle)\n",
    "  - (John Smith, lives_in, New York)\n",
    "  \n",
    "  Focus on meaningful relationships like:\n",
    "  - works_for, employed_by\n",
    "  - located_in, based_in\n",
    "  - founded_by, created_by\n",
    "  - part_of, member_of\n",
    "  - leads, manages\n",
    "  - interested_in, likes\n",
    "  - related_to, connected_to\n",
    "  \n",
    "  Extract relationships (one per line):\n",
    "  \"\"\"\n",
    "  response = llm.invoke(prompt)\n",
    "  \n",
    "  # parse relationships from response \n",
    "  relationships = []\n",
    "  lines = response.split('\\n')\n",
    "  \n",
    "  for line in lines:\n",
    "    line = line.strip()\n",
    "    # Look for patterns like (Entity1, relation, Entity2)\n",
    "    match = re.search(r'\\(([^,]+),\\s*([^,]+),\\s*([^)]+)\\)', line)\n",
    "    if match:\n",
    "      entity1 = match.group(1).strip()\n",
    "      relation = match.group(2).strip()\n",
    "      entity2 = match.group(3).strip()\n",
    "      relationships.append((entity1, relation, entity2))\n",
    "    # Also look for patterns like Entity1 -> relation -> Entity2\n",
    "    elif '->' in line:\n",
    "      parts = line.split('->')\n",
    "      if len(parts) == 3:\n",
    "        entity1 = parts[0].strip()\n",
    "        relation = parts[1].strip()\n",
    "        entity2 = parts[2].strip()\n",
    "        relationships.append((entity1, relation, entity2))\n",
    "  \n",
    "  return relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffb1f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_disambiguation(entities:List[Dict]) -> List[Dict]:\n",
    "  \"Desambigua y normaliza las entidades\"\n",
    "  if not entities:\n",
    "    return entities\n",
    "  \n",
    "  entity_str = '\\n'.join([f\"- {e['entity']} ({e['type']})\" for e in entities])\n",
    "  prompt = f\"\"\"\n",
    "  Disambiguate and normalize the following entities. \n",
    "  Remove duplicates, resolve different names for the same entity, and standardize the names.\n",
    "  \n",
    "  Entities:\n",
    "  {entity_str}\n",
    "  \n",
    "  Return the disambiguated entities in the same JSON format:\n",
    "  [\n",
    "    {{\"entity\": \"normalized_name\", \"type\": \"TYPE\"}}\n",
    "  ]\n",
    "  \n",
    "  Disambiguated entities (JSON format only):\n",
    "  \"\"\"\n",
    "  \n",
    "  response = llm.invoke(prompt)\n",
    "  \n",
    "  try:\n",
    "    json_match = re.search(r'\\[.*\\]', response, re.DOTALL)\n",
    "    if json_match:\n",
    "      disambiguated = json.loads(json_match.group())\n",
    "      return disambiguated\n",
    "  except:\n",
    "    pass\n",
    "  \n",
    "  # devuelve las entidades originales si la desambiguaci칩n falla\n",
    "  return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe885b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_triples(text:str) -> List[Tuple[str,str,str]]:\n",
    "  \"\"\"Extrae tripletas de un fragmento de texto\n",
    "\n",
    "  Args:\n",
    "      text (str): Fragmento de texto de entrada\n",
    "\n",
    "  Returns:\n",
    "      List[Tuple[str,str,str]]: Lista de tripletas (entity_1, relationship, entity_2)\n",
    "  \"\"\"\n",
    "  print(f\"Processing text chunk: {text[:100]}...\")\n",
    "  \n",
    "  # Step 1: Coreference Resolution\n",
    "  resolved_text = coreference_resolution(text)\n",
    "  print(\"Coreference resolution completed\")\n",
    "  \n",
    "  # Step 2: Named Entity Recognition\n",
    "  entities = named_entity_recognition(resolved_text)\n",
    "  print(f\"Found {len(entities)} entities\")\n",
    "  \n",
    "  # Step 3: Entity Disambiguation\n",
    "  entities = entity_disambiguation(entities)\n",
    "  print(f\"Disambiguated to {len(entities)} unique entities\")\n",
    "  \n",
    "  # Step 4: Relationship Extraction\n",
    "  relationships = relationship_extraction(resolved_text, entities)\n",
    "  print(f\"Extracted {len(relationships)} relationships\")\n",
    "  \n",
    "  return relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6605a218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(document_text:str, verbose:bool = False) -> List[Tuple[str,str,str]]:\n",
    "  \"Chunk documents + extract triples\"\n",
    "  chunks = chunk_text(document_text)\n",
    "  \n",
    "  all_triples = []\n",
    "  if verbose: print(f\"Processing document with {len(chunks)} chunks\")\n",
    "  for i, chunk in enumerate(chunks):\n",
    "    if verbose: print(f\"Processing Chunk {i+1}/{len(chunks)}\")\n",
    "    triples = extract_triples(chunk)\n",
    "    all_triples.extend(triples)\n",
    "  \n",
    "  # remover duplicados\n",
    "  unique_triples = list(set(all_triples))\n",
    "  print(f\"\\nExtracted {len(unique_triples)} unique triples from document\")\n",
    "  \n",
    "  return unique_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979e9268",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\" \n",
    "Tomaz likes to write blog posts. He is particularly interested in drawing diagrams.\n",
    "Tomaz works for Microsoft and lives in Seattle. Microsoft is a technology company \n",
    "based in Redmond, Washington. The company was founded by Bill Gates and Paul Allen.\n",
    "Tomaz's blog focuses on software engineering topics and data visualization.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f784d653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Information Extraction Pipeline ===\n",
      "\n",
      "Processing text chunk:  \n",
      "Tomaz likes to write blog posts. He is particularly interested in drawing diagrams.\n",
      "Tomaz works fo...\n",
      "Coreference resolution completed\n",
      "Found 8 entities\n",
      "Disambiguated to 8 unique entities\n",
      "Extracted 11 relationships\n",
      "\n",
      "=== EXTRACTED TRIPLES ===\n",
      "1. (Tom치s, likes, to write)\n",
      "2. (Tom치s, is, employed_by, Microsoft)\n",
      "3. (Microsoft, is, located_in, Seattle)\n",
      "4. (Microsoft, founded_by, Bill Gates)\n",
      "5. (Microsoft, founded_by, Paul Allen)\n",
      "6. (Tom치s, is, interested_in, software engineering)\n",
      "7. (Tom치s, is, interested_in, data visualization)\n",
      "8. (Tom치s, works_for, Microsoft)\n",
      "9. (Tom치s, lives_in, Seattle)\n",
      "10. (Tom치s, is, part_of, software engineering)\n",
      "11. (Tom치s, is, part_of, data visualization)\n",
      "\n",
      "=== DOCUMENT PROCESSING EXAMPLE ===\n",
      "Processing document with 2 chunks...\n",
      "\n",
      "--- Processing Chunk 1/2 ---\n",
      "Processing text chunk: John Smith is a software engineer at Google. He specializes in machine learning and artificial intel...\n",
      "Coreference resolution completed\n",
      "Found 8 entities\n",
      "Disambiguated to 7 unique entities\n",
      "Extracted 7 relationships\n",
      "\n",
      "--- Processing Chunk 2/2 ---\n",
      "Processing text chunk: has published several papers in top conferences like NIPS and ICML. His research focuses on natural ...\n",
      "Coreference resolution completed\n",
      "Found 5 entities\n",
      "Disambiguated to 5 unique entities\n",
      "Extracted 0 relationships\n",
      "\n",
      "Extracted 7 unique triples from document\n",
      "\n",
      "Extracted 7 triples from document:\n",
      "1. (John Smith, graduated_from, Stanford University)\n",
      "2. (Microsoft, located_in, Seattle)\n",
      "3. (John Smith, lives_in, New York)\n",
      "4. (Stanford University, created_by, John Smith)\n",
      "5. (John Smith, works_on, TensorFlow)\n",
      "6. (John Smith, works_for, Microsoft)\n",
      "7. (TensorFlow, started_in, 2015)\n"
     ]
    }
   ],
   "source": [
    "print(\"====== Information Extraction Pipeline ======\")\n",
    "\n",
    "# Extract triples from the sample text\n",
    "triples = extract_triples(sample_text)\n",
    "\n",
    "print(\"=== Extracted Triples ===\")\n",
    "for i, (entity1, relation, entity2) in enumerate(triples, 1): print(f\"{i}. ({entity1}, {relation}, {entity2})\")\n",
    "\n",
    "# Example with document processing\n",
    "print(\"====== Document Processing Example ======\")\n",
    "longer_document = \"\"\"\n",
    "John Smith is a software engineer at Google. He specializes in machine learning\n",
    "and artificial intelligence. Google is headquartered in Mountain View, California.\n",
    "John graduated from Stanford University with a degree in Computer Science.\n",
    "\n",
    "At Google, John works on the TensorFlow project. TensorFlow is an open-source\n",
    "machine learning framework. The project was started in 2015 and has become\n",
    "very popular among developers and researchers.\n",
    "\n",
    "John also contributes to research papers on deep learning. He has published\n",
    "several papers in top conferences like NIPS and ICML. His research focuses\n",
    "on natural language processing and computer vision.\n",
    "\"\"\"\n",
    "\n",
    "document_triples = process_document(longer_document)\n",
    "\n",
    "print(f\"Extracted {len(document_triples)} triples from document:\")\n",
    "for i, (entity1, relation, entity2) in enumerate(document_triples, 1):\n",
    "  print(f\"{i}. ({entity1}, {relation}, {entity2})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
